import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, row_number, desc
from pyspark.sql.window import Window
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType

# --- Configuration ---
if len(sys.argv) < 2:
    print("Usage: spark-submit cdc_merger_optimized.py <table_name>")
    sys.exit(1)

TARGET_TABLE_ARG = sys.argv[1]
CATALOG_NAME = "iceberg_hive"
DB_NAME = "trino_db"
TARGET_TABLE = f"cdc_{TARGET_TABLE_ARG}_optimized"
ICEBERG_TABLE = f"{CATALOG_NAME}.{DB_NAME}.{TARGET_TABLE}"
KAFKA_BOOTSTRAP_SERVERS = "kafka:9092"
KAFKA_TOPIC_PATTERN = f".*\\.public\\.{TARGET_TABLE_ARG}|.*\\.inventory\\.{TARGET_TABLE_ARG}"
CHECKPOINT_PATH = f"s3a://bucket1/checkpoints/{TARGET_TABLE}"

# Optimized Partitioning & Shuffle
SHUFFLE_PARTITIONS = 64  # Match with Kafka partitions
BATCH_SIZE = 100000      # Max offsets per micro-batch

# --- Schema Management ---
def load_dynamic_schema(table_name):
    """Loads schema from the JSON file generated by csv_to_sql.py"""
    import json
    import os
    
    schema_path = f"/home/iceberg/local/{table_name}_schema.json"
    if not os.path.exists(schema_path):
        print(f"‚ö†Ô∏è Schema file not found at {schema_path}. Falling back to default customers schema.")
        return StructType([
            StructField("payload", StructType([
                StructField("before", StructType([
                    StructField("id", IntegerType()),
                    StructField("name", StringType()),
                    StructField("email", StringType()),
                    StructField("city", StringType()),
                    StructField("updated_at", StringType())
                ])),
                StructField("after", StructType([
                    StructField("id", IntegerType()),
                    StructField("name", StringType()),
                    StructField("email", StringType()),
                    StructField("city", StringType()),
                    StructField("updated_at", StringType())
                ])),
                StructField("op", StringType())
            ]))
        ])

    with open(schema_path, 'r') as f:
        csv_schema = json.load(f)

    # Convert pandas dtypes to Spark types
    spark_fields = []
    for col_name, dtype in csv_schema.items():
        spark_type = StringType()
        if 'int' in dtype: spark_type = IntegerType()
        elif 'float' in dtype: spark_type = DoubleType()
        spark_fields.append(StructField(col_name, spark_type))

    full_struct = StructType(spark_fields)
    
    return StructType([
        StructField("payload", StructType([
            StructField("before", full_struct),
            StructField("after", full_struct),
            StructField("op", StringType())
        ]))
    ])

DYNAMIC_SCHEMA = load_dynamic_schema(TARGET_TABLE_ARG)
DATA_COLUMNS = [f.name for f in DYNAMIC_SCHEMA["payload"]["after"].dataType.fields]

def create_spark_session():
    return SparkSession.builder \
        .appName("CDC-Iceberg-Merger-Optimized") \
        .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
        .config(f"spark.sql.catalog.{CATALOG_NAME}", "org.apache.iceberg.spark.SparkCatalog") \
        .config(f"spark.sql.catalog.{CATALOG_NAME}.type", "hive") \
        .config(f"spark.sql.catalog.{CATALOG_NAME}.uri", "thrift://hive-metastore:9083") \
        .config(f"spark.sql.catalog.{CATALOG_NAME}.warehouse", "s3a://bucket1/iceberg") \
        .config(f"spark.sql.catalog.{CATALOG_NAME}.hadoop.fs.s3a.endpoint", "http://s3g:9878") \
        .config(f"spark.sql.catalog.{CATALOG_NAME}.hadoop.fs.s3a.access.key", "anyID") \
        .config(f"spark.sql.catalog.{CATALOG_NAME}.hadoop.fs.s3a.secret.key", "anySecret") \
        .config(f"spark.sql.catalog.{CATALOG_NAME}.hadoop.fs.s3a.path.style.access", "true") \
        .config(f"spark.sql.catalog.{CATALOG_NAME}.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config(f"spark.sql.catalog.{CATALOG_NAME}.hadoop.fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.access.key", "anyID") \
        .config("spark.hadoop.fs.s3a.secret.key", "anySecret") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://s3g:9878") \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.iceberg.spark.SparkSessionCatalog") \
        .config("spark.sql.catalog.spark_catalog.type", "hive") \
        .config("spark.sql.catalog.spark_catalog.uri", "thrift://hive-metastore:9083") \
        .config("spark.sql.defaultCatalog", "spark_catalog") \
        .config("spark.sql.shuffle.partitions", SHUFFLE_PARTITIONS) \
        .config("spark.streaming.backpressure.enabled", "true") \
        .config("spark.streaming.kafka.maxRatePerPartition", "5000") \
        .config(f"spark.sql.catalog.{CATALOG_NAME}.write.distribution-mode", "hash") \
        .config(f"spark.sql.catalog.{CATALOG_NAME}.write.metadata.delete-after-commit.enabled", "true") \
        .getOrCreate()

def process_batch(batch_df, batch_id):
    if batch_df.isEmpty():
        return

    print(f"üöÄ Processing Optimized Batch {batch_id} | Rows: {batch_df.count()}")
    
    # Parse and Deduplicate
    after_cols = [col(f"data.payload.after.{c}").alias(c) for c in DATA_COLUMNS]
    before_cols = [col(f"data.payload.before.{c}").alias(f"before_{c}") for c in DATA_COLUMNS]
    
    parsed_df = batch_df.select(
        col("timestamp"),
        from_json(col("value").cast("string"), DYNAMIC_SCHEMA).alias("data")
    ).select(
        col("timestamp"),
        col("data.payload.op").alias("op"),
        *after_cols,
        *before_cols
    ).selectExpr(
        f"coalesce({DATA_COLUMNS[0]}, before_{DATA_COLUMNS[0]}) as id",
        "*"
    )

    # Latest record per ID for this batch
    deduped_df = parsed_df.withColumn("rn", row_number().over(Window.partitionBy("id").orderBy(desc("timestamp")))) \
        .filter(col("rn") == 1).drop("rn", "timestamp")
    
    # Clean up 'before_' columns from the final set
    final_cols = ["id", "op"] + DATA_COLUMNS
    deduped_df = deduped_df.select(*final_cols)

    deduped_df.createOrReplaceTempView("updates")
    
    # Atomic Merge with Iceberg
    update_set = ", ".join([f"t.{c} = s.{c}" for c in DATA_COLUMNS if c != "id"])
    insert_cols = ", ".join(DATA_COLUMNS)
    insert_vals = ", ".join([f"s.{c}" for c in DATA_COLUMNS])

    batch_df.sparkSession.sql(f"""
        MERGE INTO {ICEBERG_TABLE} t
        USING updates s
        ON t.id = s.id
        WHEN MATCHED AND s.op = 'd' THEN DELETE
        WHEN MATCHED THEN UPDATE SET {update_set}
        WHEN NOT MATCHED AND s.op != 'd' THEN INSERT ({insert_cols}) VALUES ({insert_vals})
    """)

if __name__ == "__main__":
    spark = create_spark_session()
    
    # Initialize Table with Optimized Properties
    spark.sql(f"CREATE NAMESPACE IF NOT EXISTS {CATALOG_NAME}.{DB_NAME}")
    
    table_schema_sql = ", ".join([f"{f.name} {f.dataType.simpleString()}" for f in DYNAMIC_SCHEMA["payload"]["after"].dataType.fields])
    
    spark.sql(f"""
        CREATE TABLE IF NOT EXISTS {ICEBERG_TABLE} (
            {table_schema_sql}
        ) USING iceberg
        TBLPROPERTIES (
            'write.format.default'='parquet',
            'write.parquet.compression-codec'='snappy',
            'write.metadata.previous-versions-max'='10',
            'write.distribution-mode'='hash'
        )
    """)

    # Read from Kafka with High-Throughput Trigger
    raw_df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
        .option("subscribePattern", KAFKA_TOPIC_PATTERN) \
        .option("startingOffsets", "earliest") \
        .option("maxOffsetsPerTrigger", BATCH_SIZE) \
        .load()

    query = raw_df.writeStream \
        .foreachBatch(process_batch) \
        .option("checkpointLocation", CHECKPOINT_PATH) \
        .trigger(processingTime="10 seconds") \
        .start()

    print(f"üî• Optimized Merger Running for 1.6B Records. Target: {TARGET_TABLE}")
    query.awaitTermination()
